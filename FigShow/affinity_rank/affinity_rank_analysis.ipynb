{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../sup_info/crossdock2020_duplicated_uniprotId_map_smiles_in_trainset.json','r') as f:\n",
    "    crossdock2020_duplicated_uniprotId_map_smiles_in_trainset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_trainset_scaffold(x,ref_map):\n",
    "    uniprot_id  = x['UniprotID']\n",
    "    if uniprot_id not in ref_map:\n",
    "        return []\n",
    "    else:\n",
    "        all_find_scaffold = [i[1:-1] for i in x.Finded_Scaffolds[1:-1].split(', ')]\n",
    "        all_find_scaffold = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in all_find_scaffold if i != '']\n",
    "        ref_scaffolds = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in ref_map[uniprot_id]['scaffold'] if i != '']\n",
    "        list_dup_scaffolds = list(set(all_find_scaffold).intersection(set(ref_scaffolds)))\n",
    "        return list_dup_scaffolds\n",
    "def get_trainset_smiles(x,ref_map):\n",
    "    uniprot_id  = x['UniprotID']\n",
    "    if uniprot_id not in ref_map:\n",
    "        return []\n",
    "    else:\n",
    "        all_find_smiles = [i[1:-1] for i in x.Finded_Smiles[1:-1].split(', ')]\n",
    "        all_find_smiles = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in all_find_smiles if i != '']\n",
    "        ref_smiles = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in ref_map[uniprot_id]['smiles'] if i != '']\n",
    "        list_dup_smiles = list(set(all_find_smiles).intersection(set(ref_smiles)))\n",
    "    \n",
    "        return list_dup_smiles\n",
    "# map nM to pIC50\n",
    "def get_pAffinity(x):\n",
    "    '''\n",
    "    transfer nM to pIC50\n",
    "    '''\n",
    "    if not isinstance(x, (int, float)):\n",
    "        raise TypeError(\"Input Must Be a Number\")\n",
    "    x = x * 1e-9\n",
    "    \n",
    "    x = -np.log10(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get affinity map info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "affinity_info_map = defaultdict(dict)\n",
    "base_dir = '/home/datahouse1/caoduanhua/MolGens/SelfConstructedBenchmark/MolGenBench_Version1'\n",
    "for uniprot_id in tqdm(os.listdir(base_dir),total= len(os.listdir(base_dir))):\n",
    "    # serise_ids = os.listdir(os.path.join(base_dir,uniprot_id,'Hit_to_Lead_Results'))\n",
    "    # for serise_id in serise_ids:\n",
    "    ref_actives_path = os.path.join(base_dir,uniprot_id,'reference_active_molecules',f'{uniprot_id}_reference_active_molecules.sdf')\n",
    "    ref_actives = Chem.SDMolSupplier(ref_actives_path)\n",
    "    serise_id_map ={}\n",
    "    for mol in ref_actives:\n",
    "        mol_name = mol.GetProp('_Name')\n",
    "        mol_serise_id = mol_name.split('_')[1]\n",
    "        # if mol_serise_id == serise_id:\n",
    "        serise_id_string = uniprot_id + '_' + mol_serise_id\n",
    "        affinity_info_map[serise_id_string][Chem.MolToSmiles(mol)] = mol.GetProp('Affinity')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(affinity_info_map.keys())\n",
    "# all_hit2lead_serise_id_name = \n",
    "import glob \n",
    "all_hit2lead_serise_id_name = [x.split('/')[-1] for x in glob.glob(f'{base_dir}/*/Round1/Hit_to_Lead_Results/*')]\n",
    "filtered_keys = []\n",
    "for key in affinity_info_map.keys():\n",
    "    if key.split('_')[-1] in all_hit2lead_serise_id_name:\n",
    "        filtered_keys.append(key)\n",
    "print(len(filtered_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temp_func(info_map):\n",
    "    sorted_dict = {k:get_pAffinity(float(v)) for k, v in sorted(info_map.items(), key=lambda item: float(item[1]),reverse=False) if float(v) >0.0  }\n",
    "    return sorted_dict\n",
    "all_delta = [max( temp_func(affinity_info_map[key]).values()) -min( temp_func(affinity_info_map[key]).values()) for key in filtered_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affinity_info_map.keys()\n",
    "# P53779_Sries20890\n",
    "temp_dict= affinity_info_map['P53779_Sries20890']\n",
    "# 计算nM到pIC50\n",
    "def get_pAffinity(x):\n",
    "    '''\n",
    "    transfer nM to pIC50\n",
    "    '''\n",
    "    if not isinstance(x, (int, float)):\n",
    "        raise TypeError(\"Input Must Be a Number\")\n",
    "    x = x * 1e-9\n",
    "    \n",
    "    x = -np.log10(x)\n",
    "    return x\n",
    "## this version is min max normalize\n",
    "def normalize_affinty_map(info_map):\n",
    "    \"\"\"\n",
    "    Normalize the values in the dictionary to a range between 0 and 1.\n",
    "    Using max normalization.\n",
    "    Args:\n",
    "        info_map (dict): A dictionary with values to be normalized.\n",
    "    Returns:    \n",
    "        dict: A dictionary with normalized values.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Sort the dictionary by values in ascending order\n",
    "    sorted_dict = {k:get_pAffinity(float(v)) for k, v in sorted(info_map.items(), key=lambda item: float(item[1]),reverse=False) if float(v) >=0.0  }\n",
    "    \n",
    "\n",
    "    # Normalize the values\n",
    "    # min_value = min(sorted_dict.values())\n",
    "    max_value = max(sorted_dict.values())\n",
    "    min_value = min(sorted_dict.values())\n",
    "    normalized_dict = {k: ((v - min_value)/ (max_value-min_value))  for k, v in sorted_dict.items()}\n",
    "\n",
    "    return normalized_dict\n",
    "def get_find_smiles_affinity(x,affinity_info_map):\n",
    "    \"\"\"\"\n",
    "    \n",
    "    Get the affinity values for the found smiles in the input data.\n",
    "    Args:\n",
    "        x (pd.Series): A pandas Series containing 'Finded_Smiles' and ' \n",
    "        UniprotID'.\n",
    "        affinity_info_map (dict): A dictionary mapping UniprotID to another\n",
    "        dictionary of smiles to affinity values.\n",
    "    Returns:\n",
    "        list: A list of affinity values corresponding to the found smiles.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    # print(x)\n",
    "    find_smiles = x.Finded_Smiles\n",
    "    uniprot_serise_id = x.UniprotID\n",
    "    affinity_info = affinity_info_map[uniprot_serise_id]\n",
    "    affinity_info = normalize_affinty_map(affinity_info)\n",
    "    affinitys=[]\n",
    "    for smiles in find_smiles:\n",
    "        if smiles in affinity_info:\n",
    "            affinitys.append(affinity_info[smiles])\n",
    "        else:\n",
    "            affinitys.append('None')\n",
    "        \n",
    "    return affinitys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_find_smiles_rank_in_sampling\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "all_result_dir = 'the dir from hit_info_preprocess_h2l.py'\n",
    "save_dir = 'your save dir'\n",
    "for round_num in [1,2,3]:\n",
    "    all_result_path = f'{all_result_dir}/Round{round_num}'\n",
    "    all_results = os.listdir(all_result_path)\n",
    "    all_results_pd = []\n",
    "\n",
    "    for temp_path in all_results:\n",
    "        model_name = os.path.splitext(temp_path)[0]\n",
    "        temp_path = os.path.join(all_result_path, temp_path)\n",
    "        temp_pd = pd.read_csv(temp_path)\n",
    "        temp_pd['ModelName'] = [model_name]*len(temp_pd)\n",
    "        all_results_pd.append(temp_pd)\n",
    "    merged_result_pd = pd.concat(all_results_pd,axis = 0)#['ModelName'].value_counts()\n",
    "\n",
    "    merged_result_pd['Finded_Smiles'] = merged_result_pd['Finded_Smiles'].apply(lambda x: [i[1:-1] for i in x[1:-1].split(', ')])\n",
    "    merged_result_pd['Finded_Smiles_Affinity'] = merged_result_pd.apply(lambda x: get_find_smiles_affinity(x,affinity_info_map),axis = 1)\n",
    "    merged_result_pd['Finded_Smiles_Affinity'] = merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x: [i for i in x if i!='None' and i!=np.nan])\n",
    "    merged_result_pd['Finded_Smiles_Num'] = merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x : len(x))\n",
    "    merged_result_pd['Finded_Smiles_Affinity_mean'] = merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x :  np.mean( [float(i) for i in x if i != 'None']))\n",
    "    merged_result_pd['NumberHits'] =  merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x :  len([ float(i) for i in x if i != 'None']))\n",
    "    merged_result_pd['Protein_in_Crossdock2020'] = merged_result_pd['UniprotID'].apply(lambda x: x.split('_')[0] in crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)\n",
    "\n",
    "    # save all proteins\n",
    "    affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num'] > 0][['ModelName','Finded_Smiles_Affinity_mean','UniprotID','NumberHits']]\n",
    "    # affinity_pd= affinity_pd[affinity_pd['Protein_in_Crossdock2020']]\n",
    "    affinity_pd = affinity_pd.groupby('ModelName')[['UniprotID','Finded_Smiles_Affinity_mean','NumberHits']].agg(list).reset_index()\n",
    "\n",
    "    affinity_pd['Serise_Num'] = affinity_pd['NumberHits'].apply(lambda x: len(x))\n",
    "    affinity_pd['Finded_Smiles_Affinity_mean'] = affinity_pd.apply(lambda x: sum([i*j for (i,j) in zip(x.Finded_Smiles_Affinity_mean,x.NumberHits)])/sum(x.NumberHits),axis = 1)\n",
    "    affinity_pd['NumberHits_sum'] = affinity_pd['NumberHits'].apply(lambda x: np.sum(x))\n",
    "    affinity_pd['NumberHits_Mean'] = affinity_pd['NumberHits'].apply(lambda x: np.mean(x))\n",
    "    # affinity_pd\n",
    "    affinity_pd.sort_values('Finded_Smiles_Affinity_mean')[['ModelName','NumberHits_sum','Finded_Smiles_Affinity_mean','NumberHits_Mean','Serise_Num']].to_csv(f'{save_dir}/round{round_num}_all.csv')\n",
    "\n",
    "    # save seen proteins\n",
    "    affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num'] > 0][['ModelName','Finded_Smiles_Affinity_mean','UniprotID','NumberHits','Protein_in_Crossdock2020']]\n",
    "    affinity_pd= affinity_pd[affinity_pd['Protein_in_Crossdock2020']]\n",
    "    affinity_pd = affinity_pd.groupby('ModelName')[['UniprotID','Finded_Smiles_Affinity_mean','NumberHits']].agg(list).reset_index()\n",
    "\n",
    "    affinity_pd['Serise_Num'] = affinity_pd['NumberHits'].apply(lambda x: len(x))\n",
    "    affinity_pd['Finded_Smiles_Affinity_mean'] = affinity_pd.apply(lambda x: sum([i*j for (i,j) in zip(x.Finded_Smiles_Affinity_mean,x.NumberHits)])/sum(x.NumberHits),axis = 1)\n",
    "    affinity_pd['NumberHits_sum'] = affinity_pd['NumberHits'].apply(lambda x: np.sum(x))\n",
    "    affinity_pd['NumberHits_Mean'] = affinity_pd['NumberHits'].apply(lambda x: np.mean(x))\n",
    "    # affinity_pd\n",
    "    affinity_pd.sort_values('Finded_Smiles_Affinity_mean')[['ModelName','NumberHits_sum','Finded_Smiles_Affinity_mean','NumberHits_Mean','Serise_Num']].to_csv(f'{save_dir}/round{round_num}_seen.csv')\n",
    "\n",
    "    # save unseen proteins\n",
    "    affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num'] > 0][['ModelName','Finded_Smiles_Affinity_mean','UniprotID','NumberHits','Protein_in_Crossdock2020']]\n",
    "    affinity_pd= affinity_pd[~affinity_pd['Protein_in_Crossdock2020']]\n",
    "    affinity_pd = affinity_pd.groupby('ModelName')[['UniprotID','Finded_Smiles_Affinity_mean','NumberHits']].agg(list).reset_index()\n",
    "\n",
    "    affinity_pd['Serise_Num'] = affinity_pd['NumberHits'].apply(lambda x: len(x))\n",
    "    affinity_pd['Finded_Smiles_Affinity_mean'] = affinity_pd.apply(lambda x: sum([i*j for (i,j) in zip(x.Finded_Smiles_Affinity_mean,x.NumberHits)])/sum(x.NumberHits),axis = 1)\n",
    "    affinity_pd['NumberHits_sum'] = affinity_pd['NumberHits'].apply(lambda x: np.sum(x))\n",
    "    affinity_pd['NumberHits_Mean'] = affinity_pd['NumberHits'].apply(lambda x: np.mean(x))\n",
    "    # affinity_pd\n",
    "    affinity_pd.sort_values('Finded_Smiles_Affinity_mean')[['ModelName','NumberHits_sum','Finded_Smiles_Affinity_mean','NumberHits_Mean','Serise_Num']].to_csv(f'{save_dir}/round{round_num}_unseen.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_result_data(base_dir,type_name):\n",
    "\n",
    "    data_list = []\n",
    "    for round_num in [1,2,3]:\n",
    "        round_path = os.path.join(base_dir,f'round{round_num}_{type_name}.csv')\n",
    "        data = pd.read_csv(round_path)\n",
    "        data['round'] = [round_num]*len(data)\n",
    "        \n",
    "        data_list.append(\n",
    "            data\n",
    "            )\n",
    "        # [['ModelName','NumberHits_sum','Finded_Smiles_Affinity_mean','NumberHits_Mean','Serise_Num']]\n",
    "    shapemol_data = pd.DataFrame({\n",
    "        'ModelName':'ShapeMol',\n",
    "        'NumberHits_sum': [0, 0, 0],\n",
    "    'Finded_Smiles_Affinity_mean': [0, 0, 0],\n",
    "        'NumberHits_Mean': [0, 0, 0],\n",
    "        'Serise_Num': [0, 0, 0],\n",
    "        'round': [1, 2, 3]\n",
    "    })\n",
    "    # shapemol_data\n",
    "    repeats_data = pd.concat(data_list,axis = 0)\n",
    "    merged_data =repeats_data\n",
    "    # merged_data = pd.concat([repeats_data, shapemol_data], axis=0, ignore_index=True)\n",
    "\n",
    "    data = pd.DataFrame(merged_data.groupby('ModelName').agg(['mean','std']))\n",
    "    return data\n",
    "\n",
    "def plot_fig(data,x_feature,y_feature,type_name,legend=True):\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    data['modelname'] = data.index\n",
    "\n",
    "    # 创建颜色和标记映射\n",
    "    color_list = sns.color_palette(\"Set2\", n_colors=7)\n",
    "    # markers = ['o', 's', '^', 'v', 'p', '*', 'X']  # 不同的标记形状\n",
    "    markers = ['o', 'o', 'o', 'o', 'o', 'o', 'o']  # 不同的标记形状\n",
    "\n",
    "    model_names = sorted(data['modelname'].unique())\n",
    "    color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "    marker_map = {name: markers[i % len(markers)] for i, name in enumerate(model_names)}\n",
    "\n",
    "    # 为每个模型单独绘制\n",
    "    for model in model_names:\n",
    "        model_data = data[data['modelname'] == model]\n",
    "        plt.scatter(\n",
    "            x=model_data[(x_feature, 'mean')],\n",
    "            y=model_data[(y_feature, 'mean')],\n",
    "            color=color_map[model],\n",
    "            marker=marker_map[model],\n",
    "            s=200,\n",
    "            alpha=0.8,\n",
    "            edgecolor=\"w\",\n",
    "            linewidth=0.8,\n",
    "            label=model\n",
    "        )\n",
    "\n",
    "    plt.ylabel(y_feature,fontsize=14)\n",
    "    plt.xlabel('Number of Series',fontsize=14)\n",
    "    plt.title(type_name,fontsize = 14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    if legend:\n",
    "        plt.legend(fontsize =14)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'{save_dir}/hit2lead_use_ref_rank_{type_name}_protein_{y_feature}_legend.svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "    else:\n",
    "        # plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'{save_dir}/hit2lead_use_ref_rank_{type_name}_protein_{y_feature}.svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = save_dir\n",
    "# [['ModelName','NumberHits_sum','Finded_Smiles_Affinity_mean','NumberHits_Mean','Serise_Num']]\n",
    "\n",
    "x_feature_list  = ['NumberHits_sum','Serise_Num']\n",
    "y_feature_list = ['Finded_Smiles_Affinity_mean','NumberHits_Mean']\n",
    "for x_feature ,y_feature in zip(x_feature_list ,y_feature_list):\n",
    "    for type_name in ['all','seen','unseen']:\n",
    "        data = get_result_data(base_dir,type_name)\n",
    "        data.to_csv(f'{save_dir}/hit2lead_use_ref_rank_{type_name}_repeats.csv')\n",
    "        plot_fig(data,x_feature,y_feature,type_name)\n",
    "        plot_fig(data,x_feature,y_feature,type_name,legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of active smiles at different affinity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_type_data(merged_result_pd,protein_type='all'):\n",
    "    if protein_type =='all':\n",
    "        print('all')\n",
    "        \n",
    "        merge_all_affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num']>0][['UniprotID','Finded_Smiles_Affinity','Finded_Smiles_Num','ModelName']].groupby('ModelName').agg(list).reset_index()\n",
    "    elif protein_type =='seen':\n",
    "        print('seen')\n",
    "        merged_result_pd = merged_result_pd[merged_result_pd['Protein_in_Crossdock2020']]\n",
    "        merge_all_affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num']>0][['UniprotID','Finded_Smiles_Affinity','Finded_Smiles_Num','ModelName']].groupby('ModelName').agg(list).reset_index()\n",
    "    elif protein_type =='unseen':\n",
    "        print('unseen')\n",
    "        merged_result_pd = merged_result_pd[~merged_result_pd['Protein_in_Crossdock2020']]\n",
    "        merge_all_affinity_pd = merged_result_pd[merged_result_pd['Finded_Smiles_Num']>0 & ~merged_result_pd['Protein_in_Crossdock2020']][['UniprotID','Finded_Smiles_Affinity','Finded_Smiles_Num','ModelName']].groupby('ModelName').agg(list).reset_index()\n",
    "    else:\n",
    "        raise ValueError('error protein type name')\n",
    "    merge_all_affinity_pd['Finded_Smiles_Affinity_max_in_series'] = merge_all_affinity_pd['Finded_Smiles_Affinity'].apply(lambda x :[max(i) for i in x] )\n",
    "    merge_all_affinity_pd['Finded_Smiles_Affinity'] = merge_all_affinity_pd['Finded_Smiles_Affinity'].apply(lambda x :sum(x,[]) )\n",
    "    return merge_all_affinity_pd\n",
    "def get_count_data(merge_all_affinity_pd,col_name='Finded_Smiles_Affinity'):\n",
    "    # 展开数据\n",
    "    exploded_data = []\n",
    "    for index, row in merge_all_affinity_pd.iterrows():\n",
    "        model_name = row['ModelName']\n",
    "        affinity_list = row[col_name]\n",
    "        for affinity in affinity_list:\n",
    "            exploded_data.append({'ModelName': model_name, 'Affinity': affinity})\n",
    "\n",
    "    exploded_df = pd.DataFrame(exploded_data)\n",
    "\n",
    "    # 定义阈值\n",
    "    thresholds = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "    # 计算每个模型在不同阈值下的数量\n",
    "    results = []\n",
    "    models = exploded_df['ModelName'].unique()\n",
    "\n",
    "    for model in models:\n",
    "        model_data = exploded_df[exploded_df['ModelName'] == model]['Affinity']\n",
    "        total_count = len(model_data)\n",
    "        \n",
    "        # 计算大于每个阈值的数量\n",
    "        counts = {}\n",
    "        for threshold in thresholds:\n",
    "            count_above = len(model_data[model_data >= threshold])\n",
    "            counts[f'>={threshold}'] = count_above\n",
    "        \n",
    "        # 添加到结果\n",
    "        counts['Model'] = model\n",
    "        counts['Total_Count'] = total_count\n",
    "        results.append(counts)\n",
    "\n",
    "    # 转换为DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 设置模型为索引，方便绘图\n",
    "    results_df.set_index('Model', inplace=True)\n",
    "\n",
    "    # 提取数量列用于绘图\n",
    "    count_columns = [f'>={t}' for t in thresholds]\n",
    "    count_data = results_df[count_columns]\n",
    "    return count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_find_smiles_rank_in_sampling\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "merged_result_pd_list = []\n",
    "for round_num in [1,2,3]:\n",
    "    # round_num=1\n",
    "    all_result_path = f'{all_result_dir}/Round{round_num}'\n",
    "    all_results = os.listdir(all_result_path)\n",
    "    all_results_pd = []\n",
    "\n",
    "    for temp_path in all_results:\n",
    "        model_name = os.path.splitext(temp_path)[0]\n",
    "        temp_path = os.path.join(all_result_path, temp_path)\n",
    "        temp_pd = pd.read_csv(temp_path)\n",
    "        temp_pd['ModelName'] = [model_name]*len(temp_pd)\n",
    "        all_results_pd.append(temp_pd)\n",
    "    merged_result_pd = pd.concat(all_results_pd,axis = 0)#['ModelName'].value_counts()\n",
    "    merged_result_pd['Finded_Smiles'] = merged_result_pd['Finded_Smiles'].apply(lambda x: [i[1:-1] for i in x[1:-1].split(', ')])\n",
    "    # merged_result_pd['Finded_Smiles_Num'] = merged_result_pd['Finded_Smiles'].apply(lambda x : len([for i in x if i]))\n",
    "    merged_result_pd['Finded_Smiles_Affinity'] = merged_result_pd.apply(lambda x: get_find_smiles_affinity(x,affinity_info_map),axis = 1)\n",
    "    merged_result_pd['Finded_Smiles_Affinity'] = merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x: [i for i in x if i!='None' and i!=np.nan])\n",
    "    merged_result_pd['Finded_Smiles_Num'] = merged_result_pd['Finded_Smiles_Affinity'].apply(lambda x : len(x))\n",
    "\n",
    "    merged_result_pd['Protein_in_Crossdock2020'] = merged_result_pd['UniprotID'].apply(lambda x: x.split('_')[0] in crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)\n",
    "    def replace_model_name(model_name):\n",
    "        if 'diffSBDD_cond_crossdocked' in model_name:\n",
    "            return 'DiffSBDD-C'\n",
    "        elif 'diffSBDD_cond_moad' in model_name:\n",
    "            return 'DiffSBDD-M'\n",
    "        elif 'shepherd' in model_name:\n",
    "            return 'ShEPhERD'\n",
    "        elif 'DeleteHit2Lead' in model_name:\n",
    "            return 'Delete'\n",
    "        else:\n",
    "            return model_name\n",
    "    merged_result_pd['ModelName'] = merged_result_pd['ModelName'].apply(lambda x:replace_model_name(x))\n",
    "    merged_result_pd_list.append(merged_result_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data_list = []\n",
    "for round_num in [1,2,3]:\n",
    "    merge_all_affinity_pd = get_protein_type_data(merged_result_pd_list[round_num-1],protein_type='all')\n",
    "    # merge_all_affinity_pd\n",
    "    count_data = get_count_data(merge_all_affinity_pd,col_name='Finded_Smiles_Affinity')\n",
    "    count_data_list.append(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_bar(count_data,title_name='Count of Affinity Values Above Different Thresholds by Model',protein_type = 'all'):\n",
    "    # 精确控制版本\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 设置柱状图的位置和宽度\n",
    "    thresholds = [0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "    x = np.arange(len(thresholds))\n",
    "    width = 0.12  # 使用更小的宽度\n",
    "    # 计算柱子组的总宽度，确保不会超出边界\n",
    "    models = count_data.reset_index()['Model'].tolist()\n",
    "    count_columns = [f'>={t}' for t in thresholds]\n",
    "    total_width = width * len(models)\n",
    "    margin = 0.1  # 固定边距\n",
    "    # 定义背景颜色\n",
    "    background_colors = ['#f0f8ff', '#fff0f5', '#f0fff0', '#fff8dc', '#f5f5f5']\n",
    "\n",
    "    # 为每个阈值添加背景色\n",
    "    # for i, threshold in enumerate(thresholds):\n",
    "        # plt.axvspan(i - 0.5, i + 0.5, facecolor=background_colors[i % len(background_colors)], alpha=0.8)\n",
    "\n",
    "    # 为每个模型绘制一组柱状图\n",
    "    for i, model in enumerate(models):\n",
    "        model_counts = [count_data.loc[model, col] for col in count_columns]\n",
    "        # 使用固定边距\n",
    "        bars = plt.bar(x + margin + i * width, model_counts, width, label=model)\n",
    "        \n",
    "        # 在每个柱子上添加数量标签\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}',\n",
    "                    ha='center', va='bottom', fontsize=14,rotation=90)\n",
    "\n",
    "    # 在x轴下方添加模型名称标签\n",
    "    # for i, model in enumerate(models):\n",
    "    #     for j, threshold in enumerate(thresholds):\n",
    "    #         plt.text(j + margin + i * width-0.05, -max(count_data.max())*0.01,\n",
    "    #                 model,\n",
    "    #                 ha='center', va='top', fontsize=10, rotation=45)\n",
    "\n",
    "    plt.xlabel('Affinity Threshold',fontsize = 14)\n",
    "    plt.ylabel('Count',fontsize = 14)\n",
    "    plt.title(title_name,fontsize = 14)\n",
    "\n",
    "    # 调整x轴刻度位置，使其与柱子组对齐\n",
    "    plt.xticks(x + margin + width * (len(models)-1)/2, [f'≥{t}' for t in thresholds],fontsize = 14)\n",
    "\n",
    "    # 调整y轴范围，为模型名称标签留出空间\n",
    "    y_max = count_data.max().max()\n",
    "    # plt.ylim(-y_max*0.15, y_max*1.1)\n",
    "    plt.yticks([i for i in range(0,int(y_max) +20,10)],fontsize = 14)\n",
    "    # # 添加图例\n",
    "    plt.legend(loc='upper right',fontsize = 14)\n",
    "\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/{title_name}_{protein_type}.svg', bbox_inches='tight', dpi=660, format='svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for protein_type in ['all','seen','unseen']:\n",
    "    for col_name in ['Finded_Smiles_Affinity_max_in_series','Finded_Smiles_Affinity']:\n",
    "        #Finded_Smiles_Affinity_max_in_series,Finded_Smiles_Affinity\n",
    "        count_data_list = []\n",
    "        for round_num in [1,2,3]:\n",
    "            \n",
    "            merge_all_affinity_pd = get_protein_type_data(merged_result_pd_list[round_num-1],protein_type=protein_type)\n",
    "            # merge_all_affinity_pd\n",
    "            count_data = get_count_data(merge_all_affinity_pd,col_name=col_name)\n",
    "            count_data_list.append(count_data)\n",
    "        count_data_mean = pd.concat(count_data_list).groupby('Model').agg(np.mean)\n",
    "        # count_data\n",
    "        flag_string = '(Top-1 Affinity Per Series)' if col_name =='Finded_Smiles_Affinity_max_in_series' else ''\n",
    "        plot_count_bar(count_data_mean,title_name=f'Count of Affinity Values Above Different Thresholds by Model{flag_string}',protein_type = protein_type)\n",
    "        count_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbgbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
