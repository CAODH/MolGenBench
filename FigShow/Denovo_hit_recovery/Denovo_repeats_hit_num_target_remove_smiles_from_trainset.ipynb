{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../sup_info/crossdock2020_duplicated_uniprotId_map_smiles_in_trainset_inchi_map.json','r') as f:\n",
    "    crossdock2020_duplicated_uniprotId_map_smiles_in_trainset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_trainset_scaffold(x,ref_map):\n",
    "    uniprot_id  = x['UniprotID']\n",
    "    if uniprot_id not in ref_map:\n",
    "        return []\n",
    "    else:\n",
    "        all_find_inchi = [i[1:-1] for i in x.Finded_Scaffolds_Inchi[1:-1].split(', ')]\n",
    "\n",
    "        all_ref_inchi = ref_map[uniprot_id+'_scaffold'].keys()\n",
    "\n",
    "        list_dup_inchi = list(set(all_find_inchi).intersection(set(all_ref_inchi)))\n",
    "\n",
    "        list_dup_scaffolds = set([ref_map[uniprot_id+'_scaffold'][i] for i in list_dup_inchi])\n",
    "\n",
    "        return list_dup_scaffolds\n",
    "def get_trainset_smiles(x,ref_map):\n",
    "    uniprot_id  = x['UniprotID']\n",
    "    if uniprot_id not in ref_map:\n",
    "        return []\n",
    "    else:\n",
    "        all_find_inchi = [i[1:-1] for i in x.Finded_Inchi[1:-1].split(', ')]\n",
    "\n",
    "        all_ref_inchi = ref_map[uniprot_id].keys()\n",
    "        list_dup_inchi = list(set(all_find_inchi).intersection(set(all_ref_inchi)))\n",
    "        list_dup_smiles = set([ref_map[uniprot_id][i] for i in list_dup_inchi])\n",
    "    \n",
    "        return list_dup_smiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove proteins in crossdock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../sup_info/UniprotIDs_duplicated_with_crossdock2020.json','r') as f:\n",
    "    UniprotId_in_crossdock = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScaffoldAndSmilesSeen(all_result_path,UniprotId_in_crossdock,seen=True):\n",
    "    all_results = os.listdir(all_result_path)\n",
    "    all_results_pd = []\n",
    "\n",
    "    for temp_path in all_results:\n",
    "        model_name = os.path.splitext(temp_path)[0]\n",
    "        temp_path = os.path.join(all_result_path, temp_path)\n",
    "        temp_pd = pd.read_csv(temp_path)\n",
    "        temp_pd['ModelName'] = [model_name]*len(temp_pd)\n",
    "        \n",
    "        temp_pd['FindedScaffoldIsInTrainset'] = temp_pd.apply(lambda x :len(get_trainset_scaffold(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)), axis=1)\n",
    "        temp_pd['FindedSmilesIsInTrainset'] = temp_pd.apply(lambda x :len(get_trainset_smiles(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)), axis=1)\n",
    "        all_results_pd.append(temp_pd)\n",
    "        \n",
    "    merged_result_pd = pd.concat(all_results_pd,axis = 0)#['ModelName'].value_counts()\n",
    "    \n",
    "    merged_result_pd['Dupliceted_UniprotID'] = merged_result_pd['UniprotID'].apply(lambda x: x in UniprotId_in_crossdock)\n",
    "    if seen:\n",
    "        merged_result_pd = merged_result_pd[merged_result_pd['Dupliceted_UniprotID']]\n",
    "        print('Number of targets: ',len(merged_result_pd['UniprotID'].unique()))\n",
    "    else:\n",
    "        merged_result_pd = merged_result_pd[~merged_result_pd['Dupliceted_UniprotID']]\n",
    "        print('Number of targets: ',len(merged_result_pd['UniprotID'].unique()))\n",
    "    \n",
    "    \n",
    "\n",
    "    merged_result_pd['Finded_Scaffolds_Num'] = merged_result_pd['Finded_Scaffolds_Num'] - merged_result_pd['FindedScaffoldIsInTrainset']\n",
    "    merged_result_pd['Finded_Smiles_Num'] = merged_result_pd['Finded_Smiles_Num'] - merged_result_pd['FindedSmilesIsInTrainset']\n",
    "\n",
    "\n",
    "\n",
    "    scaffold_molecule_pd = merged_result_pd.groupby('ModelName').agg(list)[['Finded_Smiles_Num','Finded_Scaffolds_Num']].reset_index()#.apply(lambda x: sum(x)/120,axis = 1).plot(kind = 'bar',figsize=(10,6))\n",
    "    scaffold_molecule_pd['Mean_Finded_Scaffolds_Num'] = scaffold_molecule_pd['Finded_Scaffolds_Num'].apply(np.mean)\n",
    "    scaffold_molecule_pd['Mean_Finded_Smiles_Num'] = scaffold_molecule_pd['Finded_Smiles_Num'].apply(np.mean)\n",
    "  \n",
    "    scaffold_molecule_pd['Number_of_Finded_Scaffolds'] = scaffold_molecule_pd['Finded_Scaffolds_Num'].apply(lambda x: np.sum(np.array(x, dtype=bool)))\n",
    "    scaffold_molecule_pd['Number_of_Finded_Smiles'] = scaffold_molecule_pd['Finded_Smiles_Num'].apply(lambda x: np.sum(np.array(x, dtype=bool)))\n",
    "    return scaffold_molecule_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_path1 = '/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/Hit_Info_Results'\n",
    "\n",
    "all_result_path2 ='/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/Hit_Info_Results'\n",
    "\n",
    "all_result_path3 = '/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/Hit_Info_Results'\n",
    "scaffold_molecule_pd1_seen = getScaffoldAndSmilesSeen(all_result_path1,UniprotId_in_crossdock,seen=True)\n",
    "scaffold_molecule_pd2_seen = getScaffoldAndSmilesSeen(all_result_path2,UniprotId_in_crossdock,seen=True)\n",
    "scaffold_molecule_pd3_seen = getScaffoldAndSmilesSeen(all_result_path3,UniprotId_in_crossdock,seen=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_molecule_pd_repeats_seen = pd.concat([scaffold_molecule_pd1_seen,scaffold_molecule_pd2_seen,scaffold_molecule_pd3_seen],axis = 0)\n",
    "scaffold_molecule_pd_repeats_seen = scaffold_molecule_pd_repeats_seen[['ModelName','Number_of_Finded_Scaffolds', 'Number_of_Finded_Smiles']].groupby('ModelName').agg(['mean','std']).reset_index()\n",
    "scaffold_molecule_pd_repeats_seen.columns = ['ModelName','Number_of_Finded_Scaffolds_mean','Number_of_Finded_Scaffolds_std','Number_of_Finded_Smiles_mean','Number_of_Finded_Smiles_std']\n",
    "scaffold_molecule_pd_repeats_seen.sort_values(by = 'Number_of_Finded_Scaffolds_mean',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_molecule_pd_repeats_seen.sort_values(by = 'Number_of_Finded_Scaffolds_mean',ascending = False).to_csv('/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/mol_scaffold_recovery_seen_protein(remove smiles and scaffold in trainset).csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaffold_molecule_pd_seen = scaffold_molecule_pd_repeats_seen.sort_values(by = 'Number_of_Finded_Scaffolds_mean',ascending = False)\n",
    "plt.figure(figsize=(10,6))\n",
    "color_list = sns.color_palette(\"deep\", n_colors=10)\n",
    "model_names = sorted(scaffold_molecule_pd_seen['ModelName'].tolist())\n",
    "color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "model_colors = [color_map[name] for name in scaffold_molecule_pd_seen['ModelName']]\n",
    "bars1 = plt.bar(scaffold_molecule_pd_seen['ModelName'],scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_mean'],label = 'Number of Finded Scaffolds',alpha = 0.6, color=model_colors)\n",
    "bars2 = plt.bar(scaffold_molecule_pd_seen['ModelName'],scaffold_molecule_pd_seen['Number_of_Finded_Smiles_mean'],label = 'Number of Finded Smiles',alpha = 0.9,hatch=\"///\", color=model_colors)\n",
    "\n",
    "\n",
    "_ = plt.xticks(range(len(scaffold_molecule_pd_seen['ModelName'])), scaffold_molecule_pd_seen['ModelName'], rotation=45, ha='right')\n",
    "\n",
    "\n",
    "for bar in bars1:\n",
    "    # \n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height+2, f'{round(float(height),3)}', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height+2, f'{round(float(height),3)}', ha='center', va='bottom')\n",
    "    \n",
    "\n",
    "plt.errorbar(scaffold_molecule_pd_seen['ModelName'], scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_mean'], \n",
    "             yerr=scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_std'], fmt='none', color='blue', capsize=1,alpha = 0.5)\n",
    "\n",
    "plt.errorbar(scaffold_molecule_pd_seen['ModelName'], scaffold_molecule_pd_seen['Number_of_Finded_Smiles_mean'], \n",
    "             yerr=scaffold_molecule_pd_seen['Number_of_Finded_Smiles_std'], fmt='none', color='blue', capsize=1,alpha = 0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "plt.ylim(0, 1)  # Add 20% padding to the top\n",
    "\n",
    "plt.title('Seen Proteins(Remove Smiles and Scaffold in Trainset)')\n",
    "\n",
    "plt.ylabel('Number of Targets')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaffold_molecule_pd_seen = scaffold_molecule_pd_repeats_seen.sort_values(by = 'Number_of_Finded_Scaffolds_mean',ascending = False)\n",
    "plt.figure(figsize=(10,6))\n",
    "color_list = sns.color_palette(\"deep\", n_colors=10)\n",
    "model_names = sorted(scaffold_molecule_pd_seen['ModelName'].tolist())\n",
    "color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "model_colors = [color_map[name] for name in scaffold_molecule_pd_seen['ModelName']]\n",
    "bars1 = plt.bar(scaffold_molecule_pd_seen['ModelName'],scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_mean'],label = 'Number of Finded Scaffolds',alpha = 0.6, color=model_colors )\n",
    "bars2 = plt.bar(scaffold_molecule_pd_seen['ModelName'],scaffold_molecule_pd_seen['Number_of_Finded_Smiles_mean'],label = 'Number of Finded Smiles',alpha = 0.9,hatch=\"///\", color=model_colors )\n",
    "\n",
    "\n",
    "_ = plt.xticks(range(len(scaffold_molecule_pd_seen['ModelName'])), scaffold_molecule_pd_seen['ModelName'], rotation=45, ha='right')\n",
    "\n",
    "\n",
    "\n",
    "plt.errorbar(scaffold_molecule_pd_seen['ModelName'], scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_mean'], \n",
    "             yerr=scaffold_molecule_pd_seen['Number_of_Finded_Scaffolds_std'], fmt='none', color='blue', capsize=1,alpha = 0.5)\n",
    "\n",
    "plt.errorbar(scaffold_molecule_pd_seen['ModelName'], scaffold_molecule_pd_seen['Number_of_Finded_Smiles_mean'], \n",
    "             yerr=scaffold_molecule_pd_seen['Number_of_Finded_Smiles_std'], fmt='none', color='blue', capsize=1,alpha = 0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "plt.ylim(0, )  # Add 20% padding to the top\n",
    "# plt.ylabel('Number of Targets')\n",
    "plt.title('Seen Proteins(Remove Smiles and Scaffold in Trainset)')\n",
    "# plt.legend()\n",
    "plt.ylabel('Number of Targets')\n",
    "# plt.legend()\n",
    "plt.savefig('/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/mol_scaffold_recovery_seen_protein(remove smiles and scaffold in trainset)_no_text.svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute the similarity between trainset and generated molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_result_pd[merged_result_pd['Finded_Smiles_Num']!=0].iloc[1].Finded_Smiles[1:-1].split(', ')[0][1:-1]=='N[C@@H](Cc1ccc(O)c(O)c1)C(=O)O'\n",
    "def getScaffoldAndSmilesSeen(all_result_path,UniprotId_in_crossdock,seen=True):\n",
    "    all_results = os.listdir(all_result_path)\n",
    "    all_results_pd = []\n",
    "\n",
    "    for temp_path in all_results:\n",
    "        model_name = os.path.splitext(temp_path)[0]\n",
    "        temp_path = os.path.join(all_result_path, temp_path)\n",
    "        temp_pd = pd.read_csv(temp_path)\n",
    "        temp_pd['ModelName'] = [model_name]*len(temp_pd)\n",
    "        \n",
    "        temp_pd['FindedScaffoldIsInTrainset'] = temp_pd.apply(lambda x :len(get_trainset_scaffold(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)), axis=1)\n",
    "        temp_pd['FindedSmilesIsInTrainset'] = temp_pd.apply(lambda x :len(get_trainset_smiles(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset)), axis=1)\n",
    "        all_results_pd.append(temp_pd)\n",
    "        \n",
    "    merged_result_pd = pd.concat(all_results_pd,axis = 0)#['ModelName'].value_counts()\n",
    "    merged_result_pd['Dupliceted_UniprotID'] = merged_result_pd['UniprotID'].apply(lambda x: x in UniprotId_in_crossdock)\n",
    "    if seen:\n",
    "        merged_result_pd = merged_result_pd[merged_result_pd['Dupliceted_UniprotID']]\n",
    "        print('Number of targets: ',len(merged_result_pd['UniprotID'].unique()))\n",
    "    else:\n",
    "        merged_result_pd = merged_result_pd[~merged_result_pd['Dupliceted_UniprotID']]\n",
    "        print('Number of targets: ',len(merged_result_pd['UniprotID'].unique()))\n",
    "    \n",
    "    return merged_result_pd\n",
    "scaffold_molecule_pd1_seen = getScaffoldAndSmilesSeen(all_result_path1,UniprotId_in_crossdock,seen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_max_smi_to_trainset(x,ref_map):\n",
    "    \n",
    "    uniprot_id  = x['UniprotID']\n",
    "    ref_smiles = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in ref_map[uniprot_id].values() if i != '']\n",
    "    Finded_Smiles = [i[1:-1] for i in x.Finded_Smiles[1:-1].split(', ') if i != '']\n",
    "        # Vectorized calculation of similarities\n",
    "    ref_mols = [Chem.MolFromSmiles(smiles) for smiles in ref_smiles if smiles != '']\n",
    "    valid_mols = [mol for mol in ref_mols if mol is not None]\n",
    "    ref_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048) for mol in valid_mols]\n",
    "## 找到ref_smiles 和训练数据分子的相似性最高的分子\n",
    "    max_similarities = []\n",
    "    for ref_Finded_smiles in Finded_Smiles:\n",
    "        if ref_Finded_smiles == '':\n",
    "            # max_similarities.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        ref_Finded_mol = Chem.MolFromSmiles(ref_Finded_smiles)\n",
    "        if ref_Finded_mol is None:\n",
    "            continue\n",
    "            \n",
    "        ref_Finded_fp = AllChem.GetMorganFingerprintAsBitVect(ref_Finded_mol, 2, nBits=2048)\n",
    "        \n",
    "        similarities = [DataStructs.TanimotoSimilarity(ref_Finded_fp, fp) for fp in ref_fps]\n",
    "        \n",
    "        max_similarities.append(max(similarities))\n",
    "    \n",
    "    return max_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_id = 'P24941'\n",
    "def get_max_scaffold_smi_to_trainset(x,ref_map):\n",
    "    \n",
    "    uniprot_id  = x['UniprotID']\n",
    "    ref_smiles = [Chem.MolToSmiles(Chem.MolFromSmiles(i)) for i in ref_map[uniprot_id + '_scaffold'].values() if i != '']\n",
    "    Finded_Smiles = [i[1:-1] for i in x.Finded_Scaffolds[1:-1].split(', ') if i != '']\n",
    "        # Vectorized calculation of similarities\n",
    "    ref_mols = [Chem.MolFromSmiles(smiles) for smiles in ref_smiles if smiles != '']\n",
    "    valid_mols = [mol for mol in ref_mols if mol is not None]\n",
    "    ref_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048) for mol in valid_mols]\n",
    "## 找到ref_smiles 和训练数据分子的相似性最高的分子\n",
    "    max_similarities = []\n",
    "    for ref_Finded_smiles in Finded_Smiles:\n",
    "        if ref_Finded_smiles == '':\n",
    "            # max_similarities.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        ref_Finded_mol = Chem.MolFromSmiles(ref_Finded_smiles)\n",
    "        if ref_Finded_mol is None:\n",
    "            continue\n",
    "            \n",
    "        ref_Finded_fp = AllChem.GetMorganFingerprintAsBitVect(ref_Finded_mol, 2, nBits=2048)\n",
    "        \n",
    "        similarities = [DataStructs.TanimotoSimilarity(ref_Finded_fp, fp) for fp in ref_fps]\n",
    "        \n",
    "        max_similarities.append(max(similarities))\n",
    "    \n",
    "    return max_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_molecule_pd1_seen_find_smiles = scaffold_molecule_pd1_seen[scaffold_molecule_pd1_seen['Finded_Smiles_Num'] != 0]\n",
    "scaffold_molecule_pd1_seen_find_smiles['Max_Similarity'] = scaffold_molecule_pd1_seen_find_smiles.apply(lambda x: get_max_smi_to_trainset(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset), axis=1)\n",
    "scaffold_molecule_pd1_seen_find_smiles = scaffold_molecule_pd1_seen_find_smiles.groupby('ModelName').agg(list)[['Max_Similarity']].reset_index()#.apply(lambda x: [item for sublist in x['Max_Similarity'] for item in sublist], axis=1)\n",
    "scaffold_molecule_pd1_seen_find_smiles['Max_Similarity'] = scaffold_molecule_pd1_seen_find_smiles['Max_Similarity'].apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data for boxplot visualization\n",
    "plot_data = []\n",
    "for _, row in scaffold_molecule_pd1_seen_find_smiles.iterrows():\n",
    "    model_name = row['ModelName']\n",
    "    similarities =row['Max_Similarity']\n",
    "    for sim in similarities:\n",
    "        plot_data.append({'ModelName': model_name, 'Similarity': sim})\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Transform the data for boxplot visualization\n",
    "\n",
    "\n",
    "# Create a boxplot for the similarity data\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.boxplot(data=plot_df, x='ModelName', y='Similarity')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Distribution of Maximum Similarity to Training Set')\n",
    "plt.ylabel('Tanimoto Similarity')\n",
    "plt.xlabel('Model Name')\n",
    "\n",
    "# Add sample size annotations\n",
    "for i, model in enumerate(plot_df['ModelName'].unique()):\n",
    "    model_data = plot_df[plot_df['ModelName'] == model]\n",
    "    n = len(model_data)\n",
    "    ax.text(i, -0.1, f'n={n}', ha='center', va='top', transform=ax.get_xaxis_transform())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot for the similarity data\n",
    "plt.figure(figsize=(12, 8))\n",
    "color_list = sns.color_palette(\"deep\", n_colors=10)\n",
    "model_names = sorted(scaffold_molecule_pd_seen['ModelName'].unique().tolist())\n",
    "color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "model_colors = [color_map[name] for name in plot_df['ModelName']]\n",
    "# Create scatter plot with jitter for better visualization\n",
    "for i, model in enumerate(plot_df['ModelName'].unique()):\n",
    "    model_data = plot_df[plot_df['ModelName'] == model]['Similarity']\n",
    "    # Add small random jitter to x-coordinates for better visibility\n",
    "    x_jitter = np.random.normal(i, 0.04, size=len(model_data))\n",
    "    plt.scatter(x_jitter, model_data, alpha=0.6, s=50, color=color_map[model])\n",
    "\n",
    "plt.xticks(range(len(plot_df['ModelName'].unique())), plot_df['ModelName'].unique(), rotation=45, ha='right')\n",
    "plt.title('Distribution of Maximum Similarity to Training Set')\n",
    "plt.ylabel('Max Tanimoto Similarity between Found Smiles and Training Set')\n",
    "plt.xlabel('Model Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/mol_scaffold_recovery_seen_protein(Max Tanimoto Similarity between Found Smiles and Training Set).svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute smilarity between generated active scaffold and scaffold in trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_molecule_pd1_seen_find_scaffold = scaffold_molecule_pd1_seen[scaffold_molecule_pd1_seen['Finded_Scaffolds_Num'] != 0]\n",
    "scaffold_molecule_pd1_seen_find_scaffold['Max_Similarity'] = scaffold_molecule_pd1_seen_find_scaffold.apply(lambda x: get_max_scaffold_smi_to_trainset(x, crossdock2020_duplicated_uniprotId_map_smiles_in_trainset), axis=1)\n",
    "scaffold_molecule_pd1_seen_find_scaffold = scaffold_molecule_pd1_seen_find_scaffold.groupby('ModelName').agg(list)[['Max_Similarity']].reset_index()#.apply(lambda x: [item for sublist in x['Max_Similarity'] for item in sublist], axis=1)\n",
    "scaffold_molecule_pd1_seen_find_scaffold['Max_Similarity'] = scaffold_molecule_pd1_seen_find_scaffold['Max_Similarity'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_molecule_pd1_seen_find_scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data for boxplot visualization\n",
    "plot_data = []\n",
    "for _, row in scaffold_molecule_pd1_seen_find_scaffold.iterrows():\n",
    "    model_name = row['ModelName']\n",
    "    similarities = row['Max_Similarity']\n",
    "    for sim in similarities:\n",
    "        plot_data.append({'ModelName': model_name, 'Similarity': sim})\n",
    "\n",
    "plot_df_scaffold = pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a boxplot for the similarity data\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.boxplot(data=plot_df_scaffold, x='ModelName', y='Similarity')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Distribution of Maximum Similarity to Training Set')\n",
    "plt.ylabel('Tanimoto Similarity')\n",
    "plt.xlabel('Model Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot for the similarity data\n",
    "plt.figure(figsize=(12, 8))\n",
    "color_list = sns.color_palette(\"deep\", n_colors=10)\n",
    "model_names = sorted(scaffold_molecule_pd_seen['ModelName'].unique().tolist())\n",
    "color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "model_colors = [color_map[name] for name in plot_df['ModelName']]\n",
    "# Create scatter plot with jitter for better visualization\n",
    "for i, model in enumerate(plot_df_scaffold['ModelName'].unique()):\n",
    "    model_data = plot_df_scaffold[plot_df_scaffold['ModelName'] == model]['Similarity']\n",
    "    # Add small random jitter to x-coordinates for better visibility\n",
    "    x_jitter = np.random.normal(i, 0.04, size=len(model_data))\n",
    "    plt.scatter(x_jitter, model_data, alpha=0.6, s=50,color=color_map[model])\n",
    "\n",
    "plt.xticks(range(len(plot_df_scaffold['ModelName'].unique())), plot_df_scaffold['ModelName'].unique(), rotation=45, ha='right')\n",
    "plt.title('Distribution of Maximum Similarity to Training Set')\n",
    "plt.ylabel('Max Tanimoto Similarity between Found Scaffolds and Training Set')\n",
    "plt.xlabel('Model Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/mol_scaffold_recovery_seen_protein(Max Tanimoto Similarity between Found Scaffolds and Training Set).svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算和见过的阳性骨架一样的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "same_scaffold= plot_df_scaffold[plot_df_scaffold['Similarity']==1].groupby('ModelName').agg(list)['Similarity'].apply(lambda x: len(x)).reset_index()\n",
    "same_scaffold['all'] = plot_df_scaffold.groupby('ModelName').agg(list)['Similarity'].apply(lambda x: len(x)).reset_index()['Similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_scaffold.columns = ['ModelName','Same_Scaffold','All_Scaffold']\n",
    "same_scaffold['same_ratio'] =  same_scaffold['Same_Scaffold']/same_scaffold['All_Scaffold']\n",
    "same_scaffold = same_scaffold.sort_values(by='same_ratio', ascending=False)\n",
    "same_scaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计0.6-0.8区间的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_6_8= plot_df_scaffold[(plot_df_scaffold['Similarity']>0.6)&(plot_df_scaffold['Similarity']<0.8)].groupby('ModelName').agg(list)['Similarity'].apply(lambda x: len(x)).reset_index()\n",
    "scaffold_6_8['all'] = plot_df_scaffold.groupby('ModelName').agg(list)['Similarity'].apply(lambda x: len(x)).reset_index()['Similarity']\n",
    "scaffold_6_8.columns = ['ModelName','scaffold_6_8','All_Scaffold']\n",
    "scaffold_6_8['same_ratio'] =  scaffold_6_8['scaffold_6_8']/scaffold_6_8['All_Scaffold']\n",
    "scaffold_6_8 = scaffold_6_8.sort_values(by='same_ratio', ascending=False)\n",
    "scaffold_6_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity intervals\n",
    "intervals = [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0), (1.0, 1.0)]\n",
    "interval_labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0', '1.0']\n",
    "color_list = sns.color_palette(\"deep\", n_colors=10)\n",
    "model_names = sorted(scaffold_molecule_pd_seen['ModelName'].unique().tolist())\n",
    "color_map = {name: color_list[i % len(color_list)] for i, name in enumerate(model_names)}\n",
    "model_colors = [color_map[name] for name in plot_df['ModelName']]\n",
    "# Calculate proportions for each model and interval\n",
    "proportion_data = []\n",
    "for model in plot_df_scaffold['ModelName'].unique():\n",
    "    model_data = plot_df_scaffold[plot_df_scaffold['ModelName'] == model]['Similarity']\n",
    "    total_count = len(model_data)\n",
    "    \n",
    "    for i, (low, high) in enumerate(intervals):\n",
    "        if i == len(intervals) - 1:  # For exact 1.0 matches\n",
    "            count = sum(model_data == 1.0)\n",
    "        else:\n",
    "            count = sum((model_data >= low) & (model_data < high))\n",
    "        \n",
    "        proportion = count / total_count if total_count > 0 else 0\n",
    "        proportion_data.append({\n",
    "            'ModelName': model,\n",
    "            'Interval': interval_labels[i],\n",
    "            'Count': count,\n",
    "            'Proportion': proportion,\n",
    "            'Total': total_count\n",
    "        })\n",
    "\n",
    "proportion_df = pd.DataFrame(proportion_data)\n",
    "\n",
    "# Create a pivot table for easier plotting\n",
    "pivot_df = proportion_df.pivot(index='ModelName', columns='Interval', values='Proportion').fillna(0)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(interval_labels))\n",
    "\n",
    "# Create stacked bar chart\n",
    "bottom = np.zeros(len(pivot_df))\n",
    "bars = []\n",
    "for i, interval in enumerate(interval_labels):\n",
    "    if interval in pivot_df.columns:\n",
    "        bar = plt.bar(pivot_df.index, pivot_df[interval], bottom=bottom, \n",
    "                     label=interval, color=colors[i], alpha=0.5)\n",
    "        bars.append(bar)\n",
    "        bottom += pivot_df[interval]\n",
    "\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Distribution of Scaffold Similarity to Training Set by Intervals', pad=20)\n",
    "plt.legend(title='Similarity Intervals', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add sample size annotations\n",
    "for i, model in enumerate(pivot_df.index):\n",
    "    total = proportion_df[proportion_df['ModelName'] == model]['Total'].iloc[0]\n",
    "    plt.text(i, 1.05, f'n={total}', ha='center', va='bottom')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.text(i, -0.1, f'n={n}', ha='center', va='top', transform=plt.gca().get_xaxis_transform())\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/data-house-01/cdhofficial/MolGens/TestSample_Denovo/Round1/De_novo_Results/mol_scaffold_recovery_seen_protein(Distribution of Max Scaffold Similarity to Training Set by Intervals).svg',bbox_inches='tight',dpi=660,format='svg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolGenBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
